
# ğŸ¤– Local LLM Chatbot â€“ Powered by Ollama + Streamlit

Welcome to your very own locally running chatbot!  
This project uses [Ollama](https://ollama.com) to run large language models (like `llama3`) *fully offline* and a clean UI built with **Streamlit**.

---

## ğŸš€ What This Project Does

- ğŸ§  Loads local LLMs like `llama3` using Ollama
- ğŸ’¬ Interactive chat UI using Streamlit
- ğŸ”’ No cloud, no API keys â€” 100% private and offline

---

## ğŸ“¦ Requirements

Make sure Python 3.8+ is installed. Then:

```bash
pip install streamlit requests
```

Also make sure you have [Ollama installed](https://ollama.com/download).

Then pull a model:
```bash
ollama pull llama3
```

---

## ğŸ› ï¸ How to Run

1. Clone this repo (or copy files)
2. Create and activate a virtual environment (recommended):

```bash
python3 -m venv env
source env/bin/activate
```

3. Run the chatbot:

```bash
streamlit run app.py
```

Make sure Ollama is running in another terminal:

```bash
ollama run llama3
```

---

## ğŸ“ Project Structure

```
local-llm-chatbot/
â”‚
â”œâ”€â”€ app.py              # Streamlit UI
â”œâ”€â”€ ollama_utils.py     # LLM API interface
â”œâ”€â”€ requirements.txt    # Python dependencies
â””â”€â”€ README.md           # This file
```

---

## âœ¨ Preview

![Screenshot](https://via.placeholder.com/600x300?text=Chatbot+UI+Preview)

(*Add your own screenshot if you want â€“ looks pro!ğŸ”¥*)

---

## ğŸ“ Credits

Built by **Manu K**  
Drop a â­ if this helped you or fork it to add more features like:
- ğŸ” Streaming responses
- ğŸ­ Persona-switching bots
- ğŸ§  Memory / context windows

---

## ğŸ’¬ Chat With Me

Want to collab, ask something, or roast this bot?  
[LinkedIn](https://www.linkedin.com/in/manu-kedarasetti-b7a02112a)
